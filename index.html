<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Handspeak — Demo + main.py</title>
  <style>
    body { margin: 0; font-family: Arial, Helvetica, sans-serif; background: #0f1720; color: #e6eef6; display: flex; flex-direction: column; align-items: center; min-height:100vh; }
    header { padding: 18px; text-align:center; }
    #controls { display:flex; gap:10px; margin-bottom:8px; }
    button, a.button { background:#2563eb; border:none; color:white; padding:8px 12px; cursor:pointer; border-radius:6px; font-weight:600; text-decoration:none; display:inline-flex; align-items:center; gap:8px; }
    button.secondary { background:#475569; }
    #container { position: relative; width: 640px; margin-bottom:18px; }
    canvas { width:640px; height:480px; border-radius:6px; box-shadow:0 6px 18px rgba(0,0,0,0.6); display:block; }
    #status { position:absolute; left:8px; bottom:12px; padding:6px 10px; background: rgba(0,0,0,0.6); border-radius:6px; font-weight:bold; }
    #codeWrap { width: 90%; max-width: 900px; margin: 12px 0 36px; }
    pre { background:#071126; padding:12px; border-radius:8px; overflow:auto; max-height:420px; color:#d1e7ff; }
    footer { color:#94a3b8; margin-bottom:28px; font-size:14px; }
    .note { color:#94a3b8; font-size:13px; margin-top:6px; }
  </style>

  <!-- MediaPipe bundles -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
</head>
<body>
  <header>
    <h1 style="margin:0 0 6px 0">Handspeak — In-browser Demo</h1>
    <div class="note">This page runs the browser gesture demo immediately and includes a View/Download button for main.py from your GitHub repo.</div>
  </header>

  <div id="controls">
    <button id="startBtn">Start Camera Demo</button>
    <button id="stopBtn" class="secondary">Stop Camera</button>
    <button id="showPyBtn" class="secondary">View main.py</button>
    <a id="downloadLink" class="button" href="#" target="_blank" rel="noopener">Download main.py</a>
  </div>

  <div id="container">
    <video class="input_video" style="display:none;"></video>
    <canvas id="output_canvas" width="640" height="480"></canvas>
    <div id="status">Awaiting Gesture...</div>
  </div>

  <div id="codeWrap">
    <pre id="pyCode">Click "View main.py" to load the Python file from the repo.</pre>
  </div>

  <footer>
    Note: For camera access the page must be served from https:// or http://localhost. The Python script (main.py) uses OpenCV, MediaPipe and pyautogui and cannot run inside the browser — this page shows an equivalent browser-based demo.
  </footer>

<script>
const RAW_PY_URL = 'https://raw.githubusercontent.com/G-Manikanta1729/handspeak/96cdcc68cc8ed9fb8ee2d1ac9f0b9b8ec0b5b89e/main.py';

const canvas = document.getElementById('output_canvas');
const ctx = canvas.getContext('2d');
const statusEl = document.getElementById('status');
const pyCodeEl = document.getElementById('pyCode');
const downloadLink = document.getElementById('downloadLink');

downloadLink.href = RAW_PY_URL;
downloadLink.download = 'main.py';

// Buttons
const startBtn = document.getElementById('startBtn');
const stopBtn = document.getElementById('stopBtn');
const showPyBtn = document.getElementById('showPyBtn');

// Gesture detection helpers (same heuristic as your Python)
function isFingerUp(tip, mcp) { return tip.y < mcp.y - 0.06; }
function isFingerDown(tip, mcp) { return tip.y > mcp.y + 0.03; }

function detectGesture(landmarks) {
  const i_tip = landmarks[8], m_tip = landmarks[12], r_tip = landmarks[16], p_tip = landmarks[20];
  const i_mcp = landmarks[5], m_mcp = landmarks[9], r_mcp = landmarks[13], p_mcp = landmarks[17];

  if (isFingerUp(i_tip, i_mcp) && isFingerUp(m_tip, m_mcp) && isFingerDown(r_tip, r_mcp) && isFingerDown(p_tip, p_mcp)) return 'left';
  if (isFingerUp(i_tip, i_mcp) && isFingerUp(p_tip, p_mcp) && isFingerDown(m_tip, m_mcp) && isFingerDown(r_tip, r_mcp)) return 'right';
  if (isFingerUp(i_tip, i_mcp) && isFingerDown(m_tip, m_mcp) && isFingerDown(r_tip, r_mcp) && isFingerDown(p_tip, p_mcp)) return 'jump';
  if (isFingerDown(i_tip, i_mcp) && isFingerDown(m_tip, m_mcp) && isFingerDown(r_tip, r_mcp) && isFingerDown(p_tip, p_mcp)) return 'slide';
  return '';
}

const gestureDisplay = {
  jump: { text: 'JUMP', color: 'rgba(0,200,0,0.85)' },
  slide: { text: 'SLIDE', color: 'rgba(255,140,40,0.95)' },
  left: { text: 'LEFT', color: 'rgba(0,200,200,0.95)' },
  right: { text: 'RIGHT', color: 'rgba(200,0,0,0.95)' }
};

let prevGesture = '';
let lastTime = performance.now();
const cooldown = 800;
let cameraInstance = null;
let hands = null;

async function onResults(results) {
  ctx.save();
  ctx.clearRect(0,0,canvas.width,canvas.height);
  if (results.image) ctx.drawImage(results.image, 0, 0, canvas.width, canvas.height);

  let gesture = '';
  let color = 'rgba(150,150,150,0.6)';

  if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
    for (const landmarks of results.multiHandLandmarks) {
      window.drawConnectors(ctx, landmarks, window.HAND_CONNECTIONS, {color: '#00FF00', lineWidth: 2});
      window.drawLandmarks(ctx, landmarks, {color: '#FF2C2C', lineWidth: 1});

      const g = detectGesture(landmarks);
      if (g) { gesture = g; color = gestureDisplay[g].color; }
      break;
    }
  }

  const now = performance.now();
  if (gesture && gesture !== prevGesture && (now - lastTime) > cooldown) {
    // If you embed a same-origin game you can dispatch a custom event here:
    // window.dispatchEvent(new CustomEvent('handspeak-gesture', {detail:{gesture}}));
    prevGesture = gesture;
    lastTime = now;
  } else if (!gesture) {
    prevGesture = '';
  }

  // overlay
  ctx.fillStyle = color;
  ctx.globalAlpha = 0.15;
  ctx.fillRect(0, canvas.height - 44, canvas.width, 44);
  ctx.globalAlpha = 1;
  ctx.fillStyle = '#fff';
  ctx.font = '20px Arial';
  const statusText = gesture ? gestureDisplay[gesture].text : 'Awaiting Gesture...';
  ctx.fillText('Detected: ' + statusText, 12, canvas.height - 14);

  statusEl.textContent = gesture ? statusText : 'Awaiting Gesture...';
  statusEl.style.background = color;

  ctx.restore();
}

function startDemo() {
  if (cameraInstance) return; // already running

  hands = new Hands({
    locateFile: file => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
  });
  hands.setOptions({ maxNumHands: 1, minDetectionConfidence: 0.75, minTrackingConfidence: 0.5 });
  hands.onResults(onResults);

  const videoElement = document.getElementsByClassName('input_video')[0];
  cameraInstance = new Camera(videoElement, {
    onFrame: async () => { await hands.send({ image: videoElement }); },
    width: 640,
    height: 480
  });
  cameraInstance.start();
  startBtn.disabled = true;
  stopBtn.disabled = false;
}

function stopDemo() {
  if (!cameraInstance) return;
  cameraInstance.stop();
  cameraInstance = null;
  if (hands) { hands.close(); hands = null; }
  ctx.clearRect(0,0,canvas.width,canvas.height);
  statusEl.textContent = 'Awaiting Gesture...';
  startBtn.disabled = false;
  stopBtn.disabled = true;
}

// fetch and display main.py
async function loadMainPy() {
  try {
    pyCodeEl.textContent = 'Loading main.py...';
    const resp = await fetch(RAW_PY_URL);
    if (!resp.ok) throw new Error('Failed to fetch: ' + resp.status);
    const txt = await resp.text();
    pyCodeEl.textContent = txt;
  } catch (err) {
    pyCodeEl.textContent = 'Error loading main.py: ' + err.message;
  }
}

// Wire buttons
startBtn.addEventListener('click', startDemo);
stopBtn.addEventListener('click', stopDemo);
showPyBtn.addEventListener('click', loadMainPy);

// start disabled stop initially
stopBtn.disabled = true;

// Auto-start the demo if user clicks Start or allow auto-start by uncommenting next line (user gesture required by browsers)
//startDemo();

</script>
</body>
</html>